{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMNoTLXcs0qB"
      },
      "source": [
        "#Instaling essential libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3rvo1mSu3wh",
        "outputId": "62f33c8b-b8c9-4b01-f0a7-63e15d31e76b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: hazm in /usr/local/lib/python3.10/dist-packages (0.9.4)\n",
            "Requirement already satisfied: fasttext-wheel<0.10.0,>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from hazm) (0.9.2)\n",
            "Requirement already satisfied: flashtext<3.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from hazm) (2.7)\n",
            "Requirement already satisfied: gensim<5.0.0,>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from hazm) (4.3.2)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from hazm) (3.8.1)\n",
            "Requirement already satisfied: numpy==1.24.3 in /usr/local/lib/python3.10/dist-packages (from hazm) (1.24.3)\n",
            "Requirement already satisfied: python-crfsuite<0.10.0,>=0.9.9 in /usr/local/lib/python3.10/dist-packages (from hazm) (0.9.9)\n",
            "Requirement already satisfied: scikit-learn<2.0.0,>=1.2.2 in /usr/local/lib/python3.10/dist-packages (from hazm) (1.2.2)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.10/dist-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm) (2.11.1)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm) (67.7.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.1->hazm) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.1->hazm) (6.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (4.66.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0.0,>=1.2.2->hazm) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install hazm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DS_yedYOs5Pt"
      },
      "source": [
        "#Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6eZ-kXKo2t4s"
      },
      "outputs": [],
      "source": [
        "import hazm\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import re\n",
        "from google.colab import drive\n",
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "import string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCEMOvpXNss9"
      },
      "source": [
        "# Mutual Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ts96pa6TOOkz"
      },
      "source": [
        "**inverted index**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZFruXWvo-zs4"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def build_inverted_index(df, column_name):\n",
        "    inverted_index = defaultdict(list)\n",
        "\n",
        "    for index, row in df.head(1001).iterrows():\n",
        "        document_id = index\n",
        "        stemmed_tokens = row[column_name]\n",
        "\n",
        "        term_frequency = defaultdict(int)\n",
        "        for term in stemmed_tokens:\n",
        "            term_frequency[term] += 1\n",
        "\n",
        "        for term, frequency in term_frequency.items():\n",
        "            inverted_index[term].append((frequency, document_id))\n",
        "\n",
        "    for term, postings in inverted_index.items():\n",
        "        inverted_index[term] = sorted(postings, key=lambda x: x[0], reverse=True)\n",
        "\n",
        "    return inverted_index\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DntiIy7htD9R"
      },
      "source": [
        "#English Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OPIjmTR1l7E"
      },
      "source": [
        "**import essential libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HgtdlbJ1hzK",
        "outputId": "e6546581-f8c0-4c11-bc45-fb1d6c013cf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGZ2jNDr122n"
      },
      "source": [
        "**step 1: tokenizing words**\n",
        "\n",
        "then we tokenize the `text` column by words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SzknqC3qvsj3"
      },
      "outputs": [],
      "source": [
        "def tokenize_text(text):\n",
        "    text_no_punct = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    tokenized_text = word_tokenize(text_no_punct)\n",
        "    return tokenized_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXqmSTSB2DIS"
      },
      "source": [
        "**step 2: normalizing tokens**\n",
        "\n",
        "after tokenizing, we normalize the `tokenized_text` column. This includes lowecasing the tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mxtXl5CkyQXf"
      },
      "outputs": [],
      "source": [
        "def normalize_text(text):\n",
        "    normalized_tokens = [token.lower() for token in text]\n",
        "    return normalized_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmZD73Mq2F2k"
      },
      "source": [
        "**step 3: stemming normalized tokens**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GrCpncfYzMxE"
      },
      "outputs": [],
      "source": [
        "def stem_normalized_tokens(tokens):\n",
        "    stemmer = PorterStemmer()\n",
        "    return [stemmer.stem(token) for token in tokens]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conneting Drive to get the English dataset"
      ],
      "metadata": {
        "id": "ZN7LXg-cmslL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1Mkc8y72Yn_",
        "outputId": "95de4ba4-2765-4923-c510-9f9f1a03c0e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the shared dataset to extract 28 novels and make a dataframe out of them"
      ],
      "metadata": {
        "id": "8EiIOViZmxMW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "ffCVBxn_2koD",
        "outputId": "94d7028b-de77-4743-c114-e6b090464aa3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [novel_name, content]\n",
              "Index: []"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6a494c4c-14e2-4606-88d3-6cdaf32c68f8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>novel_name</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6a494c4c-14e2-4606-88d3-6cdaf32c68f8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6a494c4c-14e2-4606-88d3-6cdaf32c68f8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6a494c4c-14e2-4606-88d3-6cdaf32c68f8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "file_path = '/content/drive/MyDrive/Novels/'\n",
        "file_list = glob.glob(file_path + \"*\")\n",
        "\n",
        "novels_df = pd.DataFrame(columns=[\"novel_name\", \"content\"])\n",
        "novels_df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the text files and extract the content from them"
      ],
      "metadata": {
        "id": "pGyQKVM5mzFH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "bAlDWZHm8UAY"
      },
      "outputs": [],
      "source": [
        "def read_document(file_name):\n",
        "    with open(file_name) as file:\n",
        "        file_name_with_extension = os.path.basename(file_name)\n",
        "        file_name, _ = os.path.splitext(file_name_with_extension)\n",
        "        lines = file.read()\n",
        "    return file_name, lines"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to make final tokens step by step also get the total amount of tokens for each rows final tokens"
      ],
      "metadata": {
        "id": "UfhfwGU7m0y8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NJ_RuoM57_v7"
      },
      "outputs": [],
      "source": [
        "def process_all_novels(df):\n",
        "    df['tokenized_text'] = df['content'].apply(tokenize_text)\n",
        "    df['normalized_text'] = df['tokenized_text'].apply(normalize_text)\n",
        "    df['stemmed_tokens'] = df['normalized_text'].apply(stem_normalized_tokens)\n",
        "    df['final_tokens'] = df['stemmed_tokens'].apply(lambda tokens: [token for token in tokens if token and len(token) > 2])\n",
        "    df['num_tokens'] = df['final_tokens'].apply(len)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to put the content from text files into the dataframe"
      ],
      "metadata": {
        "id": "BMlY_r9im9WO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "XsG7Lsgy8nzQ"
      },
      "outputs": [],
      "source": [
        "def getting_input(df):\n",
        "  for i in range(len(file_list)):\n",
        "      file_name, content = read_document(file_list[i])\n",
        "      df.loc[i, \"novel_name\"] = file_name\n",
        "      df.loc[i, \"content\"] = content"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to add queries to the end of our dataframe so any processing would be applied to them as well"
      ],
      "metadata": {
        "id": "g63erTrJm_ZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adding_query(content, df):\n",
        "  df.loc[len(file_list), \"novel_name\"] = \"query\"\n",
        "  df.loc[len(file_list), \"content\"] = content"
      ],
      "metadata": {
        "id": "DipxF5OrdRv7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function that when called, it'll make the novels dataframe, add the query to it and get TF dataframe, IDF dataframe and by them finally get the TF-IDF dataframe and returns it"
      ],
      "metadata": {
        "id": "NrZkInObnCYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tf_idf_df_making(query,df):\n",
        "  getting_input(df)\n",
        "  adding_query(query, df)\n",
        "  process_all_novels(df)\n",
        "  inverted_index_novels = build_inverted_index(df, 'final_tokens')\n",
        "  tf_df = tf_making(inverted_index_novels, df)\n",
        "  idf_df = idf_making(inverted_index_novels, df)\n",
        "  tf_idf_df = tf_idf_making(inverted_index_novels, novels_df, tf_df, idf_df)\n",
        "  return tf_idf_df"
      ],
      "metadata": {
        "id": "HSJaA2qZtibL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function that takes the inverted index and document dataframe and create the TF dataframe"
      ],
      "metadata": {
        "id": "bsp9IWUXnD83"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WE3uHrx1ES-E"
      },
      "outputs": [],
      "source": [
        "def tf_making(inverted_index_novels, novels_df):\n",
        "  # Create an empty DataFrame to store TF values\n",
        "  tf_df = pd.DataFrame(index=inverted_index_novels.keys(), columns=novels_df['novel_name'])\n",
        "\n",
        "  # Populate the DataFrame with TF values\n",
        "  for key, values in inverted_index_novels.items():\n",
        "      for novel_name in novels_df['novel_name']:\n",
        "          document_num = novels_df.index[novels_df['novel_name'] == novel_name][0]\n",
        "          tf_value = 0\n",
        "          for value in values:\n",
        "              if value[1] == document_num:\n",
        "                  tf_value = value[0] / novels_df['num_tokens'][document_num]\n",
        "                  break  # Exit the loop once the TF value is found for the document\n",
        "          tf_df.at[key, novel_name] = tf_value\n",
        "\n",
        "  return tf_df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function that takes the inverted index and document dataframe and create the IDF dataframe"
      ],
      "metadata": {
        "id": "WgugJWSznLVX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Uvy3he5Nazvu"
      },
      "outputs": [],
      "source": [
        "def idf_making(inverted_index_novels, novels_df):\n",
        "  # Create an empty DataFrame to store IDF values\n",
        "  idf_df = pd.DataFrame(index=inverted_index_novels.keys(), columns=[0])\n",
        "\n",
        "  # Calculate the total number of documents\n",
        "  total_documents = len(novels_df)\n",
        "  # Populate the DataFrame with IDF values\n",
        "  for key, values in inverted_index_novels.items():\n",
        "      df_value = len(values)  # Document Frequency (DF) for the term\n",
        "\n",
        "      # Calculate IDF using the formula: IDF = log(total_documents / (1 + DF))\n",
        "      idf_value = np.log10(total_documents / (df_value))\n",
        "      idf_df.at[key, 0] = idf_value\n",
        "  return idf_df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function that takes the inverted index, document dataframe, TF dataframe, IDF dataframe and create the TF-IDF dataframe"
      ],
      "metadata": {
        "id": "1pE-OgHcnSCl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "7ajYxBEtfTWC"
      },
      "outputs": [],
      "source": [
        "def tf_idf_making(inverted_index_novels, novels_df, tf_df, idf_df):\n",
        "  # Create an empty DataFrame to store TF-IDF values\n",
        "  tf_idf_df = pd.DataFrame(index=novels_df['novel_name'], columns=inverted_index_novels.keys())\n",
        "\n",
        "  # Populate the DataFrame with TF-IDF values\n",
        "  for key in inverted_index_novels.keys():\n",
        "      for novel_name in novels_df['novel_name']:\n",
        "          # Retrieve TF and IDF values from precomputed DataFrames\n",
        "          tf_value = tf_df.at[key, novel_name]\n",
        "          idf_value = idf_df.at[key, 0]\n",
        "\n",
        "          # Calculate TF-IDF value\n",
        "          tf_idf_value = tf_value * idf_value\n",
        "\n",
        "          # Assign the TF-IDF value to the DataFrame\n",
        "          tf_idf_df.at[novel_name, key] = tf_idf_value\n",
        "  return tf_idf_df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to get the Cosine similarity for the query and documents then returns the 10 most similar documents"
      ],
      "metadata": {
        "id": "9nYbs9EIneMs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "4PLVNQMr7jyq"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "def get_cos_sim(df, tf_idf_df):\n",
        "  cos_sim_matrix = cosine_similarity(tf_idf_df)\n",
        "  cos_sim_df = pd.DataFrame(cos_sim_matrix, index=tf_idf_df.index, columns=tf_idf_df.index)\n",
        "  query_cosine_similarity = cos_sim_df[\"query\"]\n",
        "  top_similarities = query_cosine_similarity.nlargest(12)\n",
        "  top_similarities = top_similarities[1:11]\n",
        "  return top_similarities"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to calculate the jaccard score"
      ],
      "metadata": {
        "id": "9bgS9rornw78"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Utb14IDiX9aS"
      },
      "outputs": [],
      "source": [
        "# Calculate Jaccard similarity\n",
        "def jaccard_similarity(doc1, doc2):\n",
        "    intersection = sum((min(doc1[i], doc2[i]) for i in range(len(doc1))))\n",
        "    union = sum((max(doc1[i], doc2[i]) for i in range(len(doc1))))\n",
        "    return intersection / union"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to get the Jaccard similarity for the query and documents then returns the 10 most similar documents"
      ],
      "metadata": {
        "id": "WgsWi-bCoH-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_jac_sim(df, tf_idf_df):\n",
        "  # Create an empty DataFrame to store Jaccard similarities\n",
        "  jaccard_similarity_df = pd.DataFrame(index=tf_idf_df.index, columns=tf_idf_df.index)\n",
        "\n",
        "  # Calculate and fill in Jaccard similarities\n",
        "  for doc1 in tf_idf_df.index:\n",
        "      for doc2 in tf_idf_df.index:\n",
        "          if doc1 != doc2:\n",
        "              jaccard_similarity_df.loc[doc1, doc2] = jaccard_similarity(tf_idf_df.loc[doc1], tf_idf_df.loc[doc2])\n",
        "  jaccard_similarity_df = jaccard_similarity_df.fillna(1)\n",
        "  query_jaccard_similarity = jaccard_similarity_df[\"query\"]\n",
        "  top_similarities = query_jaccard_similarity.nlargest(12)\n",
        "  top_similarities = top_similarities[1:11]\n",
        "  return top_similarities"
      ],
      "metadata": {
        "id": "uj8oIELFyHN7"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final cell for eng dataset that gets the query from the user and print 10 most similar documents based on cosine and jaccard"
      ],
      "metadata": {
        "id": "XRv7vJ3QoLTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the query from the user\n",
        "user_query = input(\"Enter the query: \")\n",
        "tf_idf_df = tf_idf_df_making(user_query,novels_df)\n",
        "top_cos_similarities = get_cos_sim(novels_df, tf_idf_df)\n",
        "print(\"Top documents based on Cosine similarities are:\")\n",
        "print(top_cos_similarities)\n",
        "top_jac_similarities = get_jac_sim(novels_df, tf_idf_df)\n",
        "print(\"Top documents based on Jaccard similarities are:\")\n",
        "print(top_jac_similarities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ju2kqgVdtpg",
        "outputId": "dc083bef-cea3-484b-efee-a542dc500fe5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the query: no money for gambling\n",
            "Top documents based on Cosine similarities are:\n",
            "novel_name\n",
            "TheGambler                0.027273\n",
            "TheSportOfTheGods         0.004735\n",
            "ThePictureOfDorianGray    0.000925\n",
            "ARoomWithAView            0.000803\n",
            "OliverTwist               0.000633\n",
            "NightAndDay               0.000552\n",
            "TheAgeOfInnocence         0.000497\n",
            "HowardsEnd                0.000296\n",
            "TheSunAlsoRises           0.000252\n",
            "TheInvisibleMan           0.000211\n",
            "Name: query, dtype: float64\n",
            "Top documents based on Jaccard similarities are:\n",
            "novel_name\n",
            "TheGambler                0.001159\n",
            "TheSportOfTheGods         0.000342\n",
            "TheSunAlsoRises           0.000143\n",
            "HowardsEnd                0.000122\n",
            "TheInvisibleMan           0.000083\n",
            "ThePictureOfDorianGray    0.000080\n",
            "WhereAngelsFearToTread    0.000076\n",
            "ARoomWithAView            0.000067\n",
            "OliverTwist               0.000062\n",
            "HardTimes                 0.000061\n",
            "Name: query, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUbaitWDs_Wr"
      },
      "source": [
        "#Persian Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilT1F6a6pDi5"
      },
      "source": [
        "**import essential libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "RJtL5TZOr4D3"
      },
      "outputs": [],
      "source": [
        "from hazm import word_tokenize\n",
        "from hazm import Normalizer\n",
        "from hazm import Stemmer\n",
        "from hazm import stopwords_list\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download Dataset"
      ],
      "metadata": {
        "id": "KcYJO1jjpEXE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r15QOnA42v-h",
        "outputId": "5ab4b6b8-0a1d-47af-cd3a-780e66ae4aec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-01 20:07:51--  https://github.com/mohamad-dehghani/persian-pdf-books-dataset/raw/master/final_books.xlsx\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/mohamad-dehghani/persian-pdf-books-dataset/master/final_books.xlsx [following]\n",
            "--2023-12-01 20:07:51--  https://raw.githubusercontent.com/mohamad-dehghani/persian-pdf-books-dataset/master/final_books.xlsx\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1380625 (1.3M) [application/octet-stream]\n",
            "Saving to: ‘final_books.xlsx’\n",
            "\n",
            "final_books.xlsx    100%[===================>]   1.32M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-12-01 20:07:52 (27.4 MB/s) - ‘final_books.xlsx’ saved [1380625/1380625]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/mohamad-dehghani/persian-pdf-books-dataset/raw/master/final_books.xlsx"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get only the first records of dataset"
      ],
      "metadata": {
        "id": "Lm_9Lu1YpId0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "nZp4SmsMr_p9"
      },
      "outputs": [],
      "source": [
        "farsi_df = pd.read_excel('/content/final_books.xlsx')\n",
        "farsi_df = farsi_df.iloc[:100, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to add queries to the end of our dataframe so any processing would be applied to them as well"
      ],
      "metadata": {
        "id": "F7MX9aOppY_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adding_query(content, df):\n",
        "  num_rows = len(df)\n",
        "  df.loc[num_rows, \"title\"] = \"q\"\n",
        "  df.loc[num_rows, \"content\"] = content\n",
        "  df.loc[num_rows, \"date\"] = None\n",
        "  df.loc[num_rows, \"category\"] = None\n",
        "  df.loc[num_rows, \"author\"] = None\n",
        "  df.loc[num_rows, \"comments\"] = None"
      ],
      "metadata": {
        "id": "5FcPWsDUDzZ2"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function that remove punctuations"
      ],
      "metadata": {
        "id": "kL4MfYLWpaPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(text):\n",
        "    if pd.notna(text):\n",
        "        return re.sub(r'[^\\w\\s]|[.،؛]', '', text)\n",
        "    else:\n",
        "        return text"
      ],
      "metadata": {
        "id": "X0NXa4n5JT6z"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to make final tokens step by step also get the total amount of tokens for each rows final tokens"
      ],
      "metadata": {
        "id": "qTLNtHOHpe4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_all_books(df):\n",
        "  df['title'] = df['title'].apply(remove_punctuation)\n",
        "  df['content'] = df['content'].apply(remove_punctuation)\n",
        "  df['category'] = df['category'].apply(remove_punctuation)\n",
        "  df['author'] = df['author'].apply(remove_punctuation)\n",
        "\n",
        "  df['all_info'] = (\n",
        "      df['title'].fillna('') +\n",
        "      ' ' +\n",
        "      df['content'].fillna('') +\n",
        "      ' ' +\n",
        "      df['category'].fillna('') +\n",
        "      ' ' +\n",
        "      df['author'].fillna('')\n",
        "  )\n",
        "  df['tokens'] = df['all_info'].apply(lambda x: word_tokenize(x))\n",
        "  normalizer = Normalizer()\n",
        "  normalize_tokens = lambda tokens: [normalizer.normalize(token) for token in tokens]\n",
        "  df['normalized_tokens'] = df['tokens'].apply(normalize_tokens)\n",
        "\n",
        "  stemmer = Stemmer()\n",
        "  df['stemmed_tokens'] = df['normalized_tokens'].apply(lambda tokens: [stemmer.stem(token) for token in tokens])\n",
        "\n",
        "  df['final_tokens'] = df['stemmed_tokens'].apply(lambda tokens: [token for token in tokens if token and len(token) > 2])\n",
        "\n",
        "  df['num_tokens'] = df['final_tokens'].apply(len)"
      ],
      "metadata": {
        "id": "Eh39wsBrI9bh"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDi4IcBWzzEa"
      },
      "source": [
        "Function that takes the inverted index and document dataframe and create the TF dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "kNzOTcDyxsMq"
      },
      "outputs": [],
      "source": [
        "def tf_making(inverted_index_novels, farsi_df):\n",
        "    tf_values = np.zeros((len(inverted_index_novels), len(farsi_df)))\n",
        "\n",
        "    for i, (term, postings) in enumerate(inverted_index_novels.items()):\n",
        "        for frequency, document_id in postings:\n",
        "            tf_values[i, document_id] = frequency / farsi_df['num_tokens'][document_id]\n",
        "\n",
        "    tf_df = pd.DataFrame(tf_values, index=inverted_index_novels.keys(), columns=farsi_df['title'])\n",
        "    return tf_df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function that takes the inverted index and document dataframe and create the IDF dataframe"
      ],
      "metadata": {
        "id": "teO0R7Tbpnki"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "iEZ4OyqG2LTT"
      },
      "outputs": [],
      "source": [
        "# IDF Making\n",
        "def idf_making(inverted_index_novels, farsi_df):\n",
        "    total_documents = len(farsi_df)\n",
        "    df_values = np.array([len(postings) for postings in inverted_index_novels.values()])\n",
        "    idf_values = np.log10(total_documents / (1 + df_values))\n",
        "\n",
        "    idf_df = pd.DataFrame(idf_values, index=inverted_index_novels.keys(), columns=[0])\n",
        "    return idf_df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function that takes the inverted index, document dataframe, TF dataframe, IDF dataframe and create the TF-IDF dataframe"
      ],
      "metadata": {
        "id": "rfaDbAfEpqo5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "PuaEV2mkn5ck"
      },
      "outputs": [],
      "source": [
        "# TF-IDF Making\n",
        "def tf_idf_making(inverted_index_novels, farsi_df, tf_df, idf_df):\n",
        "    # Convert TF DataFrame to sparse matrix\n",
        "    tf_matrix = csr_matrix(tf_df.values)\n",
        "\n",
        "    # Use sklearn's TfidfTransformer for efficient TF-IDF computation\n",
        "    transformer = TfidfTransformer(norm=None, smooth_idf=False)\n",
        "    tf_idf_matrix = transformer.fit_transform(tf_matrix.transpose())\n",
        "\n",
        "    # Convert the result back to a DataFrame\n",
        "    tf_idf_df = pd.DataFrame(tf_idf_matrix.toarray(), index=farsi_df['title'], columns=inverted_index_novels.keys())\n",
        "\n",
        "    return tf_idf_df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to get the Cosine similarity for the query and documents then returns the 10 most similar documents"
      ],
      "metadata": {
        "id": "WWez89VOps3T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "gotA7Ze3oddo"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "def get_cos_sim(df, tf_idf_df):\n",
        "  cos_sim_matrix = cosine_similarity(tf_idf_df)\n",
        "  cos_sim_df = pd.DataFrame(cos_sim_matrix, index=tf_idf_df.index, columns=tf_idf_df.index)\n",
        "  query_cosine_similarity = cos_sim_df[\"q\"]\n",
        "  top_similarities = query_cosine_similarity.nlargest(12)\n",
        "  top_similarities = top_similarities[1:11]\n",
        "  return top_similarities"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to calculate the jaccard score"
      ],
      "metadata": {
        "id": "NiPKya4tpvHN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "TAs2d_8DohKK"
      },
      "outputs": [],
      "source": [
        "# Calculate Jaccard similarity\n",
        "def jaccard_similarity(doc1, doc2):\n",
        "    intersection = sum((min(doc1[i], doc2[i]) for i in range(len(doc1))))\n",
        "    union = sum((max(doc1[i], doc2[i]) for i in range(len(doc1))))\n",
        "    return intersection / union if union != 0 else 0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to get the Jaccard similarity for the query and documents then returns the 10 most similar documents"
      ],
      "metadata": {
        "id": "ufoCuUz1pwm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_jac_sim(tf_idf_df):\n",
        "  # Create an empty DataFrame to store Jaccard similarities\n",
        "  jaccard_similarity_df = pd.DataFrame(index=tf_idf_df.index, columns=tf_idf_df.index)\n",
        "\n",
        "  for i in range(len(tf_idf_df)):\n",
        "    for j in range(i + 1, len(tf_idf_df)):\n",
        "        doc1 = tf_idf_df.index[i]\n",
        "        doc2 = tf_idf_df.index[j]\n",
        "        jaccard_sim = jaccard_similarity(tf_idf_df.loc[doc1], tf_idf_df.loc[doc2])\n",
        "        jaccard_similarity_df.loc[doc1, doc2] = jaccard_sim\n",
        "        jaccard_similarity_df.loc[doc2, doc1] = jaccard_sim\n",
        "\n",
        "  jaccard_similarity_df = jaccard_similarity_df.fillna(1)\n",
        "  query_jaccard_similarity = jaccard_similarity_df[\"q\"]\n",
        "  top_similarities = query_jaccard_similarity.nlargest(12)\n",
        "  top_similarities = top_similarities[1:11]\n",
        "  return top_similarities"
      ],
      "metadata": {
        "id": "pun0bRgci6V-"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final cell for eng dataset that gets the query from the user and print 10 most similar documents based on cosine and jaccard"
      ],
      "metadata": {
        "id": "12LpZfCBpyz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "import numpy as np\n",
        "\n",
        "# Get the query from the user\n",
        "user_query = input(\"Enter the query: \")\n",
        "adding_query(user_query, farsi_df)\n",
        "process_all_books(farsi_df)\n",
        "\n",
        "# Remove unnecessary columns\n",
        "columns_to_remove = ['content', 'date', 'comments', 'category', 'author', 'all_info', 'tokens', 'normalized_tokens', 'stemmed_tokens']\n",
        "farsi_df = farsi_df.drop(columns=columns_to_remove, axis=1)\n",
        "\n",
        "# Build Inverted Index\n",
        "inverted_index_novels = build_inverted_index(farsi_df, 'final_tokens')\n",
        "\n",
        "# TF-IDF Making\n",
        "tf_df = tf_making(inverted_index_novels, farsi_df)\n",
        "idf_df = idf_making(inverted_index_novels, farsi_df)\n",
        "tf_idf_df = tf_idf_making(inverted_index_novels, farsi_df, tf_df, idf_df)\n",
        "\n",
        "top_cos_similarities = get_cos_sim(farsi_df, tf_idf_df)\n",
        "print(\"Top documents based on Cosine similarities are:\")\n",
        "print(top_cos_similarities)\n",
        "top_jac_similarities = get_jac_sim(tf_idf_df)\n",
        "print(\"Top documents based on Jaccard similarities are:\")\n",
        "print(top_jac_similarities)"
      ],
      "metadata": {
        "id": "EAFK22kyDpJc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe2dae37-9d14-4569-e3e0-ad380b3b1a40"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the query: ایران در عهد باستان\n",
            "Top documents based on Cosine similarities are:\n",
            "title\n",
            " ایلخانان ایران                         0.098678\n",
            " برگ های تاریخ  دوران قاجاریه           0.092825\n",
            " تاجگذاری شاهنشاهان ایران               0.086702\n",
            " آسمان پرستاره                          0.082854\n",
            " از سلاجقه تا صفویه                     0.069310\n",
            " رضا شاه                                0.062348\n",
            " از قاجار به پهلوی                      0.058149\n",
            " سکه پژوهی                              0.044037\n",
            " تاریخ سلطانی  از شیخ صفی تا شاه صفی    0.041708\n",
            " سرگذشت رضاشاه                          0.040519\n",
            "Name: q, dtype: float64\n",
            "Top documents based on Jaccard similarities are:\n",
            "title\n",
            " تاجگذاری شاهنشاهان ایران        0.023711\n",
            " ایلخانان ایران                  0.018356\n",
            " برگ های تاریخ  دوران قاجاریه    0.014389\n",
            " آسمان پرستاره                   0.011445\n",
            " رضا شاه                         0.011199\n",
            " از سلاجقه تا صفویه              0.010834\n",
            " از قاجار به پهلوی               0.009559\n",
            " افسردگی را بخوریم               0.006997\n",
            " سرگذشت رضاشاه                   0.006964\n",
            " سکه پژوهی                       0.006367\n",
            "Name: q, dtype: float64\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}